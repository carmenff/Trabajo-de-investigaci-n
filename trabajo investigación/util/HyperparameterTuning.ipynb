{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import scipy \n",
    "import util\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "import math\n",
    "import time\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_DATA = \"E:Corrected_FA/ALL_DATA/\"\n",
    "info_data = \"idaSearch_8_01_2020.csv\"\n",
    "\n",
    "# Obtenemos los diccionarios con los nombres de los ficheros que contienen las imágenes\n",
    "AD_CN, groups = util.obtain_data_files(ALL_DATA, info_data)\n",
    "\n",
    "# Cargamos las imágenes\n",
    "CN_imgs = np.array(util.load_data(ALL_DATA, AD_CN[\"CN\"]), dtype='float32')\n",
    "\n",
    "AD_imgs = util.load_data(ALL_DATA, AD_CN[\"AD\"])\n",
    "\n",
    "# Extendemos la clase con menos ejemplos\n",
    "AD_imgs = np.array(util.extend_class(AD_imgs, len(CN_imgs)), dtype='float32')\n",
    "\n",
    "# Creamos las etiquetas 1: AD, 0:CN\n",
    "CN_labels = np.zeros((len(CN_imgs),1), dtype = \"int32\")\n",
    "AD_labels = np.ones((len(AD_imgs),1), dtype = \"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función para crear el modelo seleccionando número de filtros y neuronas, dropout rate y parámetro de regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(filters, neurons, dropout_rate, regularization):\n",
    "    layers = tf.keras.layers\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv3D(filters, 11, strides = (4,4,4), padding= 'valid', input_shape=(91,109,91, 1), activation = 'relu'),\n",
    "        layers.BatchNormalization(),     \n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        layers.Conv3D(filters, 5, strides = (1,1,1), padding= 'valid', activation = 'relu'),\n",
    "        layers.BatchNormalization(),    \n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        layers.Conv3D(filters, 3, strides = (1,1,1), padding= 'valid', activation = 'relu'),\n",
    "        layers.BatchNormalization(),    \n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        layers.Conv3D(filters, 3, strides = (1,1,1), padding= 'valid', activation = 'relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        layers.Conv3D(filters, 3, strides = (1,1,1), padding= 'valid', activation = 'relu'),    \n",
    "        layers.GlobalAveragePooling3D(),\n",
    "\n",
    "        layers.Dense(neurons, activation = \"relu\", activity_regularizer= keras.regularizers.l2(regularization)),\n",
    "        layers.Dense(neurons, activation = \"relu\", activity_regularizer= keras.regularizers.l2(regularization)),\n",
    "        layers.Dense(neurons, activation = \"relu\", activity_regularizer= keras.regularizers.l2(regularization)),\n",
    "        layers.Dense(1, activation = 'sigmoid')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función para seleccionar únicamente dropout rate y regularización.\n",
    "\n",
    "Esta es la función que utilicé finalmente ya que utilizamos el número de filtros y neuronas de VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_def(dropout, regularization):\n",
    "    layers = tf.keras.layers\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv3D(64, 11, strides = (4,4,4), padding= 'valid', input_shape=(91,109,91, 1)),\n",
    "        layers.BatchNormalization(),    \n",
    "        layers.ReLU(),       \n",
    "\n",
    "        layers.Conv3D(128, 5, strides = (1,1,1), padding= 'valid'),\n",
    "        layers.BatchNormalization(),    \n",
    "        layers.ReLU(),\n",
    "\n",
    "        layers.Conv3D(256, 3, strides = (1,1,1), padding= 'valid'),\n",
    "        layers.BatchNormalization(),    \n",
    "        layers.ReLU(),\n",
    "\n",
    "        layers.Conv3D(512, 3, strides = (1,1,1), padding= 'valid'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.MaxPooling3D(),\n",
    "\n",
    "        layers.Conv3D(512, 3, strides = (1,1,1), padding= 'valid'),    \n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.GlobalAveragePooling3D(),\n",
    "\n",
    "        layers.Dense(512, activation = \"relu\"),\n",
    "        layers.Dropout(dropout),\n",
    "\n",
    "        layers.Dense(512, activation = \"relu\"),\n",
    "        layers.Dropout(dropout),\n",
    "\n",
    "        layers.Dense(512, activation = \"relu\"),\n",
    "        layers.Dropout(dropout),\n",
    "\n",
    "        layers.Dense(1, activation = 'sigmoid')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones para probar un modelo\n",
    "Modelo dado número de filtros y neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_model(train_ds, val_ds, train_size, filters, neurons, batch_size = 32, dropout=0.05, reg=0.003, learning_rate = 3e-7, fold = 0, n_epoch = 200):\n",
    "    \"\"\" Crea, compila y entrena un modelo con los parámetros obtenidos, además guarda el modelo con mejor loss y el modelo con mejor accuracy.\n",
    "    Devuelve la evaluación del modelo con mejor loss y la del modelo con mejor accuracy, los path de los modelos y el history del entrenamiento\"\"\"\n",
    "    \n",
    "    loss_path = \"model_loss_{}_{}_{}_{}_{}.h5\".format(filters,neurons, dropout, reg, fold)\n",
    "    \n",
    "    checkpoint_cb_loss = keras.callbacks.ModelCheckpoint(loss_path, monitor=\"val_loss\", save_best_only = True) \n",
    "    \n",
    "    root_logdir = os.path.join(os.curdir, \"my_logs_cv\") \n",
    "    def get_run_logdir(): \n",
    "        run_id = \"run_{}_{}_{}_{}_{}\".format(filters,neurons, dropout, reg, fold) \n",
    "        return os.path.join(root_logdir, run_id) \n",
    "    \n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(get_run_logdir())\n",
    "\n",
    "    # Se crea el modelo\n",
    "    m = create_model_def(dropout, reg)\n",
    "    # Se compila\n",
    "    m.compile(optimizer = keras.optimizers.Adam(learning_rate), loss = tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])       \n",
    "    # Se entrena\n",
    "    history = m.fit(train_ds.repeat(), epochs = n_epoch, steps_per_epoch= train_size/batch_size, \n",
    "                    validation_data = val_ds, verbose = 0, callbacks =[checkpoint_cb_loss, \n",
    "                                                                       tensorboard_cb]) \n",
    "    # Evaluacion del modelo con mejor loss\n",
    "    m = keras.models.load_model(loss_path) \n",
    "    evaluation_loss = m.evaluate(val_ds)\n",
    "    \n",
    "    \n",
    "    return {\"ev_loss\": evaluation_loss, \"loss_path\": loss_path, \"history\": history}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo dado dropout rate y regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_model_final(train_ds, val_ds, train_size, batch_size = 32, dropout=0.05, reg=0.003, learning_rate = 3e-7, fold = 0, n_epoch = 200):\n",
    "    \"\"\" Crea, compila y entrena un modelo con los parámetros obtenidos, además guarda el modelo con mejor loss y el modelo con mejor accuracy.\n",
    "    Devuelve la evaluación del modelo con mejor loss y la del modelo con mejor accuracy, los path de los modelos y el history del entrenamiento\"\"\"\n",
    "    \n",
    "    loss_path = \"model_loss_{}_{}_{}.h5\".format(dropout, reg, fold)\n",
    "    \n",
    "    checkpoint_cb_loss = keras.callbacks.ModelCheckpoint(loss_path, monitor=\"val_loss\", save_best_only = True) \n",
    "    \n",
    "    root_logdir = os.path.join(os.curdir, \"my_logs_cv\") \n",
    "    def get_run_logdir(): \n",
    "        run_id = \"run__{}_{}_{}\".format(dropout, reg, fold) \n",
    "        return os.path.join(root_logdir, run_id) \n",
    "    \n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(get_run_logdir())\n",
    "\n",
    "    # Se crea el modelo\n",
    "    m = create_model_def(dropout, reg)\n",
    "    # Se compila\n",
    "    m.compile(optimizer = keras.optimizers.Adam(learning_rate), loss = tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])       \n",
    "    # Se entrena\n",
    "    history = m.fit(train_ds.repeat(), epochs = n_epoch, steps_per_epoch= train_size/batch_size, \n",
    "                    validation_data = val_ds, verbose = 0, callbacks =[checkpoint_cb_loss, \n",
    "                                                                       tensorboard_cb]) \n",
    "    # Evaluacion del modelo con mejor loss\n",
    "    m = keras.models.load_model(loss_path) \n",
    "    evaluation_loss = m.evaluate(val_ds)\n",
    "    \n",
    "    \n",
    "    return {\"ev_loss\": evaluation_loss, \"loss_path\": loss_path, \"history\": history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "data = util.train_test_split(CN_imgs, CN_labels, AD_imgs, AD_labels, 0.15)\n",
    "\n",
    "CN_imgs, AD_imgs = None, None # Liberamos memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dropouts = [0.2, .3,.4,.5]\n",
    "n_epoch = 250\n",
    "batch_size = 32\n",
    "n_folds = 5\n",
    "effective_folds = 3\n",
    "histories = []\n",
    "\n",
    "best_model_loss = \"\"\n",
    "best_loss = np.inf\n",
    "best_acc = 0\n",
    "dropout = None\n",
    "reg = 0.003\n",
    "lr = 1e-6\n",
    "\n",
    "\n",
    "best_loss_parameters = []\n",
    "\n",
    "for d in dropouts:    \n",
    "    run_evaluations = []\n",
    "    for fold in range(effective_folds):\n",
    "        start = time.time()\n",
    "        #print(\"Iniciado modelo con f = {} y n = {}\".format(f,n))\n",
    "\n",
    "        fold_data = util.k_fold(data[\"train_imgs\"], data[\"train_labels\"], n_folds, fold )\n",
    "        train_ds = fold_data[\"train_ds\"].map(lambda tensor, labels : util.transform(tensor,labels), num_parallel_calls=16)\\\n",
    "                                        .batch(batch_size).prefetch(8)\n",
    "        val_ds = fold_data[\"val_ds\"].batch(fold_data[\"val_size\"])\n",
    "        train_size = fold_data[\"train_size\"]\n",
    "\n",
    "        evaluations = try_model_final(train_ds, val_ds, train_size, dropout = d, fold= fold, learning_rate = lr, n_epoch = n_epoch)\n",
    "        evaluation_loss = evaluations[\"ev_loss\"]\n",
    "        history = evaluations[\"history\"]\n",
    "\n",
    "        print(\"{} dropout, {} regularization, {} fold\".format(d, reg, fold))\n",
    "        print(\"Loss: {}, Accuracy: {}\".format(evaluation_loss[0], evaluation_loss[1]))\n",
    "\n",
    "        histories.append(history)\n",
    "        run_evaluations.append(evaluation_loss)\n",
    "        end = time.time()\n",
    "        print(\"Tiempo de ejecucion de fold: {}\".format(end-start))\n",
    "\n",
    "    # Comprobamos si el modelo con mejor loss es el mejor hasta el momento\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    for i in range(len(run_evaluations)):\n",
    "        loss += run_evaluations[i][0]\n",
    "        acc += run_evaluations[i][1]\n",
    "    loss /= len(run_evaluations)\n",
    "    acc /= len(run_evaluations)\n",
    "\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_model_loss = evaluations[\"loss_path\"]\n",
    "        print(\"Nuevo mejor modelo de loss con {}\".format(best_loss))\n",
    "\n",
    "    if acc > best_acc:\n",
    "\n",
    "        best_acc = acc\n",
    "        best_model_acc = evaluations[\"loss_path\"]\n",
    "        print(\"Nuevo mejor modelo de accuracy con {}\".format(best_acc))\n",
    "\n",
    "    print(\"Definitive evaluation of best loss of model with {} dropout, {} regularization, {} fold\".format(dropout, reg, fold))\n",
    "    print(\"Loss: {}, Accuracy: {}\".format(loss, acc))\n",
    "\n",
    "        \n",
    "print(\"Terminado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
